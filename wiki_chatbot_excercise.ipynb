{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wiki_chatbot_excercise.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOQ-NHhqYQiD",
        "colab_type": "text"
      },
      "source": [
        "# Asking ChatBot about chosen information\n",
        "\n",
        "\n",
        "*   What is..\n",
        "*   Tell me about..\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGpl2sGvbSFO",
        "colab_type": "text"
      },
      "source": [
        "## Libraries or concepts used in the process\n",
        " \n",
        "*   NLTK - a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.\n",
        "\n",
        "*   TF-IDF - statistical method of evaluating the significance of a word in a given document.\n",
        "\n",
        "*   Cosine similarity - denotes the similarity between the two words\n",
        "\n",
        "*   WordNet -  a lexical database for the English language. It groups English words into sets of synonyms called synsets, provides short definitions and usage examples, and records a number of relations among these synonym sets or their members.\n",
        "\n",
        "*   WordNetLemmatizer -  Lemmatize using WordNet's built-in morphy function. Lemmatization \n",
        "\n",
        "*   Wikipedia - Python library that makes it easy to access and parse data from Wikipedia.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0dtRZxYZlz0",
        "colab_type": "text"
      },
      "source": [
        "## Installs libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ln_LkfvPZBgi",
        "colab_type": "code",
        "outputId": "3ad16c9b-6a4d-4af0-ba6e-087c3cd6242a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!pip install wikipedia"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading https://files.pythonhosted.org/packages/67/35/25e68fbc99e672127cc6fbb14b8ec1ba3dfef035bf1e4c90f78f24a80b7d/wikipedia-1.4.0.tar.gz\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (2.21.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2019.11.28)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-cp36-none-any.whl size=11686 sha256=de052de9a2ab296ff71d695b9003d94c25b717a65ce88f55efa04ea0409832a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/2a/18/4e471fd96d12114d16fe4a446d00c3b38fb9efcb744bd31f4a\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8vYVLMGMszd",
        "colab_type": "text"
      },
      "source": [
        "## Imports libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoLUbJY4fTgF",
        "colab_type": "code",
        "outputId": "639044a2-752b-4534-b494-822795d39a57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "import nltk\n",
        "import random\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import re, string, unicodedata\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import wikipedia as wk\n",
        "from collections import defaultdict \n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "nltk.download('punkt') \n",
        "nltk.download('wordnet')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel\n",
        "import urllib.request\n",
        "import re\n",
        "from IPython.display import Image"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLnFchprfPFL",
        "colab_type": "text"
      },
      "source": [
        "## Gets text data from url and cleans it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2--IZ-k7AcrK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "''' TODO: Use urllib.request module with urlopen function to get data from url'''\n",
        "# uf = (\"https://plato.stanford.edu/entries/meaning/\")\n",
        "html = uf.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAvefLrppdw5",
        "colab_type": "code",
        "outputId": "8f740af7-9cb7-4cfb-9bc4-ab75951b58c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "html[:100]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'<!DOCTYPE html>\\n<!--[if lt IE 7]> <html class=\"ie6 ie\"> <![endif]-->\\n<!--[if IE 7]>    <html class=\"'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7ABljy9-YDv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove html tags and unnecessary characters\n",
        "def cleanhtml(raw_html):\n",
        "  cleanr = re.compile('\\n|<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
        "  cleantext = re.sub(cleanr, '', raw_html)\n",
        "  return cleantext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jh7ykxP9_PKt",
        "colab_type": "code",
        "outputId": "342d7bf9-dda6-4746-8534-e2232a8f2a7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "'''TODO: change byte to string using decode() and utf-8 format'''\n",
        "# html = html.\n",
        "html[:100]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<!DOCTYPE html>\\n<!--[if lt IE 7]> <html class=\"ie6 ie\"> <![endif]-->\\n<!--[if IE 7]>    <html class=\"'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RV5KNvx8rU7S",
        "colab_type": "text"
      },
      "source": [
        "## Remove html tags and remove capital letters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOBLhu99qmKm",
        "colab_type": "code",
        "outputId": "47f1fa1a-5427-4dff-9198-2462946873f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "'''TODO: use previously defined function cleanhtml and remove capital letters with lower()'''\n",
        "# html = \n",
        "print(html[:100])\n",
        "# raw = html.\n",
        "print(raw[:100])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                  -->  Theories of Meaning (Stanford Encyclopedia of Philosophy)                    \n",
            "                  -->  theories of meaning (stanford encyclopedia of philosophy)                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRZDuOpVaF4J",
        "colab_type": "text"
      },
      "source": [
        "## Sentence tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q55VS2-2Nfe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''TODO: use nltk function sent_tokenize on raw'''\n",
        "# sent_tokens = "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyA9w_TE_6F-",
        "colab_type": "code",
        "outputId": "7228b8c9-cf37-40c8-983d-1f898de015b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "sent_tokens[1:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['unfortunately, this term has also been used to mean a greatnumber of different things.',\n",
              " 'in this entry, the focus is on two sortsof theory of meaning.',\n",
              " 'the first sort of theoryasemantic theoryis a theory which assigns semantic contents toexpressions of a language.',\n",
              " 'the second sort of theoryafoundational theory of meaningis a theory which states thefacts in virtue of which expressions have the semantic contents thatthey have.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpLM0OFFaHBM",
        "colab_type": "text"
      },
      "source": [
        "## Text normalisation\n",
        "\n",
        "Word tokenization\n",
        "\n",
        "*   Tokenization is the act of breaking up a sequence of strings into pieces such as words, keywords, phrases, symbols and other elements called tokens\n",
        "\n",
        "*   Tokens can be individual words, phrases or even whole sentences. In the process of tokenization, some characters like punctuation marks are discarded\n",
        "\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPv_-9-L_CXe",
        "colab_type": "text"
      },
      "source": [
        "## Working with Unicode\n",
        "Strings are usually easy to deal with when they are made up of English ASCII characters, but “problems” appear when we enter into non-ASCII characters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBtDihvx_6LK",
        "colab_type": "text"
      },
      "source": [
        "### What are strings made of?\n",
        "Byte is a unit of information that is built of 8 bits — bytes are used to store all files in a hard disk. So all of the CSVs and JSON files on your computer are built of bytes. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HadI0Xdx_6su",
        "colab_type": "text"
      },
      "source": [
        "### ASCII\n",
        "\n",
        "\n",
        "\n",
        "* character encoding standard\n",
        "* 127 symbol list \n",
        "* cool for the initial few decades or so\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQv3WHreACcK",
        "colab_type": "text"
      },
      "source": [
        "### Unicode\n",
        "\n",
        "* International standard where a mapping of individual characters and a unique number is maintained\n",
        "* Over 137k characters including different scripts including English, Hindi, Chinese and Japanese, as well as emojis\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ic_vpFXACwV",
        "colab_type": "text"
      },
      "source": [
        "### Unicode encodings UTF-8, UTF-16, and UTF-32\n",
        "\n",
        "\n",
        "*   UTF-8: It uses 1, 2, 3 or 4 bytes to encode every code point\n",
        "*   UTF-16 is variable 2 or 4 bytes, great for Asian text\n",
        "* UTF-32 is fixed 4 bytes, needs a lot of memory, not used very often\n",
        "\n",
        "decode() -> str <br>\n",
        "encode() -> bytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfXDd9FcADGY",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://cdn.bulldogjob.com/system/photos/files/000/005/268/original/1_nyvQSXsxG7cZILqZ8H5-Wg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WM57N5MO_7F3",
        "colab_type": "text"
      },
      "source": [
        "### Example of encoding and decoding\n",
        "\n",
        "#### unicodedata.normalize() - specification by the Unicode character encoding standard that some sequences of code points represent essentially the same character"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDEookpoiTvc",
        "colab_type": "code",
        "outputId": "98a2d5b4-2ea1-48ba-cf9f-2b1f865f9c90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "word = \"pythön\"\n",
        "# unicodedata normalize to return the normal form\n",
        "print(unicodedata.normalize('NFKD', word))\n",
        "print(unicodedata.normalize('NFKD', word).encode('ascii', 'ignore'))\n",
        "print(unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore'))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pythön\n",
            "b'python'\n",
            "python\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0Y7MNwOuSqf",
        "colab_type": "code",
        "outputId": "7131cf9e-b062-44da-9014-b33b64a99246",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "word = \"pythön\"\n",
        "# ignore means that we do not replace odd character with anything\n",
        "print(word.encode('ascii', 'ignore'))\n",
        "print(word.encode('ascii', 'ignore').decode('utf-8', 'ignore'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'pythn'\n",
            "pythn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwmXC1-WMVvS",
        "colab_type": "text"
      },
      "source": [
        "## Text normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aP_T5kGM5aCX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Normalize(text):\n",
        "    remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "    #word tokenization\n",
        "    word_token = nltk.word_tokenize(text.lower().translate(remove_punct_dict))\n",
        "\n",
        "    #remove ascii\n",
        "    new_words = []\n",
        "    for word in word_token:\n",
        "      '''TODO: use unicodedata.normalize module with 'NFKD' as form and encode with ascii and decode with utf-8'''\n",
        "        #new_word = \n",
        "        new_words.append(new_word)\n",
        "\n",
        "    #remove tags\n",
        "    rmv = []\n",
        "    for w in new_words:\n",
        "        text=re.sub(\"&lt;/?.*?&gt;\",\"&lt;&gt;\",w)\n",
        "        rmv.append(text)\n",
        "        \n",
        "    #pos tagging and lemmatization\n",
        "    tag_map = defaultdict(lambda : wn.NOUN)\n",
        "    tag_map['J'] = wn.ADJ\n",
        "    tag_map['V'] = wn.VERB\n",
        "    tag_map['R'] = wn.ADV\n",
        "    lmtzr = WordNetLemmatizer()\n",
        "    lemma_list = []\n",
        "    rmv = [i for i in rmv if i]\n",
        "    for token, tag in nltk.pos_tag(rmv):\n",
        "        lemma = lmtzr.lemmatize(token, tag_map[tag[0]])\n",
        "        lemma_list.append(lemma)\n",
        "    return lemma_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQLOr9rZaIBQ",
        "colab_type": "text"
      },
      "source": [
        "## Creating greeting responses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ShXA9BM5aPA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "''' TODO: defining welcome input that will be recognized by bot '''\n",
        "# welcome_input = [\"Hi\", \"Hello\", ..]\n",
        "\n",
        "''' TODO: defining welcome output from bot '''\n",
        "# welcome_response = [\"*nods*\", \"hi there\", ..]\n",
        "\n",
        "def welcome(user_response):\n",
        "    for word in user_response.split():\n",
        "        if word.lower() in welcome_input:\n",
        "''' TODO: Choose by random bot answer using library random and function choice()'''\n",
        "          #  return "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z23iUKMPaIwY",
        "colab_type": "text"
      },
      "source": [
        "## Generating response for the knowledge question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PZnJeiy5aZG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generateResponse(user_response):\n",
        "    robo_response=''\n",
        "    sent_tokens.append(user_response)\n",
        "    TfidfVec = TfidfVectorizer(tokenizer=Normalize, stop_words='english')\n",
        "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
        "    #vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "    vals = linear_kernel(tfidf[-1], tfidf)\n",
        "    idx=vals.argsort()[0][-2]\n",
        "    flat = vals.flatten()\n",
        "    flat.sort()\n",
        "    req_tfidf = flat[-2]\n",
        "    if(req_tfidf==0) or \"tell me about\" in user_response:\n",
        "        print(\"Checking Wikipedia\")\n",
        "        if user_response:\n",
        "            robo_response = wikipedia_data(user_response)\n",
        "            return robo_response\n",
        "    else:\n",
        "        robo_response = robo_response+sent_tokens[idx]\n",
        "        return robo_response\n",
        "        #wikipedia search\n",
        "def wikipedia_data(input):\n",
        "    reg_ex = re.search('tell me about (.*)', input)\n",
        "    try:\n",
        "        if reg_ex:\n",
        "            topic = reg_ex.group(1)\n",
        "            wiki = wk.summary(topic, sentences = 3)\n",
        "            return wiki\n",
        "    except Exception as e:\n",
        "            print(\"No content has been found\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WsSQ-QTaJny",
        "colab_type": "text"
      },
      "source": [
        "## Running the bot while True"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ux_NlXFw5amZ",
        "colab_type": "code",
        "outputId": "13ca0c1c-e407-4c06-99dc-0c0230c528c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        }
      },
      "source": [
        "flag=True\n",
        "print(\"My name is Chatterbot and I'm a chatbot. If you want to exit, type Bye!\")\n",
        "while(flag==True):\n",
        "    user_response = input()\n",
        "    # changing input to lowercase\n",
        "    user_response=user_response.lower()\n",
        "    # checking if the user want to exit\n",
        "    if(user_response not in ['bye','shutdown','exit', 'quit']):\n",
        "        if(user_response=='thanks' or user_response=='thank you' ):\n",
        "            flag=False\n",
        "            print(\"Chatterbot : You are welcome..\")\n",
        "        else:\n",
        "            if user_response in welcome_input:\n",
        "                print(\"Chatterbot : \"+welcome(user_response))\n",
        "            else:\n",
        "                print(\"Chatterbot : \",end=\"\")\n",
        "                print(generateResponse(user_response))\n",
        "                sent_tokens.remove(user_response)\n",
        "    else:\n",
        "        flag=False\n",
        "        print(\"Chatterbot : Bye!!! \")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "My name is Chatterbot and I'm a chatbot. If you want to exit, type Bye!\n",
            "tell me about human\n",
            "Chatterbot : Checking Wikipedia\n",
            "Humans (Homo sapiens) are the only extant members of the subtribe Hominina.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \"\"\"\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-b001f3567577>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"My name is Chatterbot and I'm a chatbot. If you want to exit, type Bye!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0muser_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m# changing input to lowercase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0muser_response\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         )\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5UZi8rZm04Y",
        "colab_type": "text"
      },
      "source": [
        "### Ask what is:\n",
        "\n",
        "*   What is theory of meaning\n",
        "*   What is possible worlds semantics\n",
        "\n",
        "### Ask to tell:\n",
        "\n",
        "*   Tell me about nothing\n",
        "*   Tell me about human\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vJmfj1o5a9a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Sources: https://towardsdatascience.com/lets-build-an-intelligent-chatbot-7ea7f215ada6,\n",
        "https://towardsdatascience.com/a-guide-to-unicode-utf-8-and-strings-in-python-757a232db95c'''"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}