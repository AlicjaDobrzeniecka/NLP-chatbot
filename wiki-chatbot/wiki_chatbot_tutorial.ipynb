{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wiki_chatbot_tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOQ-NHhqYQiD",
        "colab_type": "text"
      },
      "source": [
        "# Asking ChatBot about chosen information\n",
        "\n",
        "\n",
        "*   What is..\n",
        "*   Tell me about..\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGpl2sGvbSFO",
        "colab_type": "text"
      },
      "source": [
        "## Libraries or concepts used in the process\n",
        " \n",
        "*   NLTK - a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.\n",
        "\n",
        "*   TF-IDF - statistical method of evaluating the significance of a word in a given document.\n",
        "\n",
        "*   Cosine similarity - denotes the similarity between the two words\n",
        "\n",
        "*   WordNet -  a lexical database for the English language. It groups English words into sets of synonyms called synsets, provides short definitions and usage examples, and records a number of relations among these synonym sets or their members.\n",
        "\n",
        "*   WordNetLemmatizer -  Lemmatize using WordNet's built-in morphy function. Lemmatization \n",
        "\n",
        "*   Wikipedia - Python library that makes it easy to access and parse data from Wikipedia.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0dtRZxYZlz0",
        "colab_type": "text"
      },
      "source": [
        "## Installs libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ln_LkfvPZBgi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "aa40a50c-d2df-4b00-94d4-a90004c05de7"
      },
      "source": [
        "!pip install wikipedia"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading https://files.pythonhosted.org/packages/67/35/25e68fbc99e672127cc6fbb14b8ec1ba3dfef035bf1e4c90f78f24a80b7d/wikipedia-1.4.0.tar.gz\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (2.21.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2019.11.28)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-cp36-none-any.whl size=11686 sha256=e658a3e1db4701b84b0abad64c238fa99305eebd45aa31a4c4a3d3591d26bc45\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/2a/18/4e471fd96d12114d16fe4a446d00c3b38fb9efcb744bd31f4a\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8vYVLMGMszd",
        "colab_type": "text"
      },
      "source": [
        "## Imports libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoLUbJY4fTgF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "857e7e46-263f-47c8-f342-a09ef9a76afa"
      },
      "source": [
        "import nltk\n",
        "import random\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import re, string, unicodedata\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import wikipedia as wk\n",
        "from collections import defaultdict \n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "nltk.download('punkt') \n",
        "nltk.download('wordnet')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel\n",
        "import urllib.request\n",
        "import re\n",
        "from IPython.display import Image"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLnFchprfPFL",
        "colab_type": "text"
      },
      "source": [
        "## Gets text data from url and cleans it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2--IZ-k7AcrK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "uf = urllib.request.urlopen(\"https://plato.stanford.edu/entries/meaning/\")\n",
        "html = uf.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAvefLrppdw5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "788acecb-b0b4-4ded-c153-4d76f330fe83"
      },
      "source": [
        "html[:100]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'<!DOCTYPE html>\\n<!--[if lt IE 7]> <html class=\"ie6 ie\"> <![endif]-->\\n<!--[if IE 7]>    <html class=\"'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7ABljy9-YDv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove html tags and unnecessary characters\n",
        "def cleanhtml(raw_html):\n",
        "  cleanr = re.compile('\\n|<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
        "  cleantext = re.sub(cleanr, '', raw_html)\n",
        "  return cleantext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jh7ykxP9_PKt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "537a0593-522c-4362-85bf-60f653d7d569"
      },
      "source": [
        "# change to string\n",
        "html = html.decode(\"utf-8\")\n",
        "html[:100]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<!DOCTYPE html>\\n<!--[if lt IE 7]> <html class=\"ie6 ie\"> <![endif]-->\\n<!--[if IE 7]>    <html class=\"'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RV5KNvx8rU7S",
        "colab_type": "text"
      },
      "source": [
        "## Shows the beginning of the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOBLhu99qmKm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "2de80e4c-ec51-4105-8e80-2ad349ee1083"
      },
      "source": [
        "html = cleanhtml(html)\n",
        "print(html[:100])\n",
        "raw = html.lower()\n",
        "print(raw[:100])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                  -->  Theories of Meaning (Stanford Encyclopedia of Philosophy)                    \n",
            "                  -->  theories of meaning (stanford encyclopedia of philosophy)                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRZDuOpVaF4J",
        "colab_type": "text"
      },
      "source": [
        "## Sentence tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q55VS2-2Nfe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent_tokens = nltk.sent_tokenize(raw)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyA9w_TE_6F-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "183c61e4-5664-4d5c-ae83-b7d8a21f2611"
      },
      "source": [
        "sent_tokens[1:5]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['unfortunately, this term has also been used to mean a greatnumber of different things.',\n",
              " 'in this entry, the focus is on two sortsof theory of meaning.',\n",
              " 'the first sort of theoryasemantic theoryis a theory which assigns semantic contents toexpressions of a language.',\n",
              " 'the second sort of theoryafoundational theory of meaningis a theory which states thefacts in virtue of which expressions have the semantic contents thatthey have.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpLM0OFFaHBM",
        "colab_type": "text"
      },
      "source": [
        "## Text normalisation\n",
        "\n",
        "Word tokenization\n",
        "\n",
        "*   Tokenization is the act of breaking up a sequence of strings into pieces such as words, keywords, phrases, symbols and other elements called tokens\n",
        "\n",
        "*   Tokens can be individual words, phrases or even whole sentences. In the process of tokenization, some characters like punctuation marks are discarded\n",
        "\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPv_-9-L_CXe",
        "colab_type": "text"
      },
      "source": [
        "## Working with Unicode\n",
        "Strings are usually easy to deal with when they are made up of English ASCII characters, but “problems” appear when we enter into non-ASCII characters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBtDihvx_6LK",
        "colab_type": "text"
      },
      "source": [
        "### What are strings made of?\n",
        "Byte is a unit of information that is built of 8 bits — bytes are used to store all files in a hard disk. So all of the CSVs and JSON files on your computer are built of bytes. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HadI0Xdx_6su",
        "colab_type": "text"
      },
      "source": [
        "### ASCII\n",
        "\n",
        "\n",
        "\n",
        "* character encoding standard\n",
        "* 127 symbol list \n",
        "* cool for the initial few decades or so\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQv3WHreACcK",
        "colab_type": "text"
      },
      "source": [
        "### Unicode\n",
        "\n",
        "* International standard where a mapping of individual characters and a unique number is maintained\n",
        "* Over 137k characters including different scripts including English, Hindi, Chinese and Japanese, as well as emojis\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ic_vpFXACwV",
        "colab_type": "text"
      },
      "source": [
        "### Unicode encodings UTF-8, UTF-16, and UTF-32\n",
        "\n",
        "\n",
        "*   UTF-8: It uses 1, 2, 3 or 4 bytes to encode every code point\n",
        "*   UTF-16 is variable 2 or 4 bytes, great for Asian text\n",
        "* UTF-32 is fixed 4 bytes, needs a lot of memory, not used very often\n",
        "\n",
        "decode() -> str <br>\n",
        "encode() -> bytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfXDd9FcADGY",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://cdn.bulldogjob.com/system/photos/files/000/005/268/original/1_nyvQSXsxG7cZILqZ8H5-Wg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WM57N5MO_7F3",
        "colab_type": "text"
      },
      "source": [
        "### Example of encoding and decoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDEookpoiTvc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "2f756af6-47ab-46dc-b4e3-4fd5511b3d03"
      },
      "source": [
        "word = \"pythön\"\n",
        "# unicodedata normalize return the normal form\n",
        "print(unicodedata.normalize('NFKD', word).encode('ascii', 'ignore'))\n",
        "print(unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore'))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'python'\n",
            "python\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0Y7MNwOuSqf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "8c26084b-5d32-4853-e0c8-97d479da2364"
      },
      "source": [
        "word = \"pythön\"\n",
        "# ignore means that we do not replace odd character with anything\n",
        "print(word.encode('ascii', 'ignore'))\n",
        "print(word.encode('ascii', 'ignore').decode('utf-8', 'ignore'))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'pythn'\n",
            "pythn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwmXC1-WMVvS",
        "colab_type": "text"
      },
      "source": [
        "## Text normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aP_T5kGM5aCX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Normalize(text):\n",
        "    remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "    #word tokenization\n",
        "    word_token = nltk.word_tokenize(text.lower().translate(remove_punct_dict))\n",
        "\n",
        "    #remove ascii\n",
        "    new_words = []\n",
        "    for word in word_token:\n",
        "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "        new_words.append(new_word)\n",
        "\n",
        "    #remove tags\n",
        "    rmv = []\n",
        "    for w in new_words:\n",
        "        text=re.sub(\"&lt;/?.*?&gt;\",\"&lt;&gt;\",w)\n",
        "        rmv.append(text)\n",
        "        \n",
        "    #pos tagging and lemmatization\n",
        "    tag_map = defaultdict(lambda : wn.NOUN)\n",
        "    tag_map['J'] = wn.ADJ\n",
        "    tag_map['V'] = wn.VERB\n",
        "    tag_map['R'] = wn.ADV\n",
        "    lmtzr = WordNetLemmatizer()\n",
        "    lemma_list = []\n",
        "    rmv = [i for i in rmv if i]\n",
        "    for token, tag in nltk.pos_tag(rmv):\n",
        "        lemma = lmtzr.lemmatize(token, tag_map[tag[0]])\n",
        "        lemma_list.append(lemma)\n",
        "    return lemma_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQLOr9rZaIBQ",
        "colab_type": "text"
      },
      "source": [
        "## Creating greeting responses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ShXA9BM5aPA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# defining welcome input that will be recognized by bot\n",
        "welcome_input = [\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\"]\n",
        "# defining welcome output from bot \n",
        "welcome_response = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
        "\n",
        "def welcome(user_response):\n",
        "    for word in user_response.split():\n",
        "        if word.lower() in welcome_input:\n",
        "            return random.choice(welcome_response)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z23iUKMPaIwY",
        "colab_type": "text"
      },
      "source": [
        "## Generating response for the knowledge question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PZnJeiy5aZG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generateResponse(user_response):\n",
        "    robo_response=''\n",
        "    sent_tokens.append(user_response)\n",
        "    TfidfVec = TfidfVectorizer(tokenizer=Normalize, stop_words='english')\n",
        "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
        "    #vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "    vals = linear_kernel(tfidf[-1], tfidf)\n",
        "    idx=vals.argsort()[0][-2]\n",
        "    flat = vals.flatten()\n",
        "    flat.sort()\n",
        "    req_tfidf = flat[-2]\n",
        "    if(req_tfidf==0) or \"tell me about\" in user_response:\n",
        "        print(\"Checking Wikipedia\")\n",
        "        if user_response:\n",
        "            robo_response = wikipedia_data(user_response)\n",
        "            return robo_response\n",
        "    else:\n",
        "        robo_response = robo_response+sent_tokens[idx]\n",
        "        return robo_response\n",
        "        #wikipedia search\n",
        "def wikipedia_data(input):\n",
        "    reg_ex = re.search('tell me about (.*)', input)\n",
        "    try:\n",
        "        if reg_ex:\n",
        "            topic = reg_ex.group(1)\n",
        "            wiki = wk.summary(topic, sentences = 3)\n",
        "            return wiki\n",
        "    except Exception as e:\n",
        "            print(\"No content has been found\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WsSQ-QTaJny",
        "colab_type": "text"
      },
      "source": [
        "## Running the bot while True"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ux_NlXFw5amZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "outputId": "9d8ecffd-e1c6-4f34-c062-bbdd87ae96df"
      },
      "source": [
        "flag=True\n",
        "print(\"My name is Chatterbot and I'm a chatbot. If you want to exit, type Bye!\")\n",
        "while(flag==True):\n",
        "    user_response = input()\n",
        "    # changing input to lowercase\n",
        "    user_response=user_response.lower()\n",
        "    # checking if the user want to exit\n",
        "    if(user_response not in ['bye','shutdown','exit', 'quit']):\n",
        "        if(user_response=='thanks' or user_response=='thank you' ):\n",
        "            flag=False\n",
        "            print(\"Chatterbot : You are welcome..\")\n",
        "        else:\n",
        "            if user_response in welcome_input:\n",
        "                print(\"Chatterbot : \"+welcome(user_response))\n",
        "            else:\n",
        "                print(\"Chatterbot : \",end=\"\")\n",
        "                print(generateResponse(user_response))\n",
        "                sent_tokens.remove(user_response)\n",
        "    else:\n",
        "        flag=False\n",
        "        print(\"Chatterbot : Bye!!! \")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "My name is Chatterbot and I'm a chatbot. If you want to exit, type Bye!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5UZi8rZm04Y",
        "colab_type": "text"
      },
      "source": [
        "### Ask what is:\n",
        "\n",
        "*   What is theory of meaning\n",
        "*   What is possible worlds semantics\n",
        "\n",
        "### Ask to tell:\n",
        "\n",
        "*   Tell me about nothing\n",
        "*   Tell me about human\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vJmfj1o5a9a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Sources: https://towardsdatascience.com/lets-build-an-intelligent-chatbot-7ea7f215ada6,\n",
        "https://towardsdatascience.com/a-guide-to-unicode-utf-8-and-strings-in-python-757a232db95c'''"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}