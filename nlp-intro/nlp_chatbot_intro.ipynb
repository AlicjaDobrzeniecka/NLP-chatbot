{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "nlp_chatbot_intro",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvwebI2d9Tuk",
        "colab_type": "text"
      },
      "source": [
        "# Natural language processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SppmYyFG9Tus",
        "colab_type": "text"
      },
      "source": [
        "### Plan warsztatu:\n",
        "\n",
        "\n",
        "*   Wstęp teoretyczny\n",
        "*   Implementacja prostego chatbota\n",
        "*   Chatbot - ćwiczenie"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6hVRR5z9Tuv",
        "colab_type": "text"
      },
      "source": [
        "## Co to jest i do czego służy?\n",
        "### Przetwarzanie języka naturalnego\n",
        "\n",
        "\n",
        "*   interdyscyplinarna dziedzina, łącząca zagadnienia sztucznej inteligencji i językoznawstwa\n",
        "*   zajmuje się automatyzacją analizy, rozumienia, tłumaczenia i generowania języka naturalnego przez komputer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bJk7Q-oHG60",
        "colab_type": "text"
      },
      "source": [
        "## Proces analizy tekstu\n",
        "\n",
        "\n",
        "\n",
        "1. Analiza leksykalna - proces przetwarzania sekwencji znaków w sekwencję tokenów\n",
        "\n",
        "2. Analiza składniowa - proces analizy tekstu, w celu ustalenia jego struktury gramatycznej i zgodności z gramatyką język\n",
        "\n",
        "3. Analiza semantyczna - analiza znaczeń\n",
        "\n",
        "4. Wynik końcowy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSXsIxcuG366",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://witanworld.com/wp-content/uploads/2018/05/Natural-Language-Processing-steps.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUKxMEoq9Tuz",
        "colab_type": "text"
      },
      "source": [
        "## Tekst wejściowy\n",
        "#### London is the capital and most populous city of England and the United Kingdom. Standing on the River Thames in the south east of the island of Great Britain, London has been a major settlement for two millennia. It was founded by the Romans, who named it Londinium.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ta6hvOXl9Tu3",
        "colab_type": "text"
      },
      "source": [
        "## Segmentacja tekstu\n",
        "#### “London is the capital and most populous city of England and the United Kingdom.”\n",
        "#### “Standing on the River Thames in the south east of the island of Great Britain, London has been a major settlement for two millennia.”\n",
        "#### “It was founded by the Romans, who named it Londinium.”"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoR_U-rT9Tu8",
        "colab_type": "text"
      },
      "source": [
        "## Tokenizacja\n",
        "#### “London”, “is”, “ the”, “capital”, “and”, “most”, “populous”, “city”, “of”, “England”, “and”, “the”, “United”, “Kingdom”"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQlPSWhk9Tu_",
        "colab_type": "text"
      },
      "source": [
        "## Lematyzacja\n",
        "#### “London”, “be” , “ the”, “capital”, “and”, “most”, “populous”, “city”, “of”, “England”, “and”, “the”, “United”, “Kingdom”"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XO8-zy09TvD",
        "colab_type": "text"
      },
      "source": [
        "## Identyfikacja słów nieważnych (ang. Stop-words)\n",
        "#### “London”, “capital”, “populous”, “city”, “England”, “United”, “Kingdom”"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-CWPyC89TvG",
        "colab_type": "text"
      },
      "source": [
        "## Importujemy potrzebne biblioteki"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eq47Cq8A9TvI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import random\n",
        "import string \n",
        "import pandas as pd\n",
        "import requests"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECEYoyWE9TvR",
        "colab_type": "text"
      },
      "source": [
        "#### lower() - pozbywamy się wielkich liter\n",
        "#### wordnet - baza danych słów w języku agielskim\n",
        "#### punkt - token pozwalający na podział tekstu na zdania\n",
        "#### sent_tokenize() - funkcja, która pozwala nam na zamiane tekstu w liste zdań\n",
        "#### word_tokenize() -  funkcja, która pozwala nam na zamiane zdań w liste słów"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwu3GOtqzjlB",
        "colab_type": "text"
      },
      "source": [
        "## Load text data from github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2FmKZdSxyKN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/AlicjaDobrzeniecka/NLP-chatbot/master/chatbot.txt'\n",
        "r = requests.get(url)\n",
        "raw = r.content.decode(\"utf-8\") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaSxxOuq9TvV",
        "colab_type": "code",
        "outputId": "841ee316-390a-4577-8e20-dd54924da438",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "nltk.download('punkt') # first-time use only\n",
        "nltk.download('wordnet') # first-time use only\n",
        "\n",
        "sent_tokens = nltk.sent_tokenize(raw)\n",
        "word_tokens = nltk.word_tokenize(raw)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ut_DHjcR9Tvf",
        "colab_type": "code",
        "outputId": "4ba86893-b8e3-4667-ba14-44ff206aeabe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "sent_tokens"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A chatbot is a piece of software that conducts a conversation via auditory or textual methods.',\n",
              " '[1] Such programs are often designed to convincingly simulate how a human would behave as a conversational partner, although as of 2019, they are far short of being able to pass the Turing test.',\n",
              " '[2] Chatbots are typically used in dialog systems for various practical purposes including customer service or information acquisition.',\n",
              " 'Some chatbots use sophisticated natural language processing systems, but many simpler ones scan for keywords within the input, then pull a reply with the most matching keywords, or the most similar wording pattern, from a database.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PI142UTI9Tvm",
        "colab_type": "code",
        "outputId": "8a146a2e-9a64-4a13-f5cc-ac56ae65900e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "word_tokens[1:5]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['chatbot', 'is', 'a', 'piece']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0OEuydK9Tvt",
        "colab_type": "text"
      },
      "source": [
        "##### We shall now define a function called LemTokens which will take as input the tokens and return normalized tokens.\n",
        "\n",
        "##### Aby dokonać lematyzacji, należy pobrać WordNetLematizer() oraz stworzyć jego instancję.\n",
        "##### Następnie wywołujemy funkcję lemmatize(), jako argument podajemy pojedyncze słowo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gU8rQeVA9Tvv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        " \n",
        "def LemTokens(words):\n",
        "    return [lemmer.lemmatize(word) for word in words]\n",
        "\n",
        "# translation table - a dictionary\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "\n",
        "def LemNormalize(text):\n",
        "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHtoZilY9Tv3",
        "colab_type": "code",
        "outputId": "2ddcb56e-352f-491e-9f67-fe34ec9c49cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "LemTokens(sent_tokens)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A chatbot is a piece of software that conducts a conversation via auditory or textual methods.',\n",
              " '[1] Such programs are often designed to convincingly simulate how a human would behave as a conversational partner, although as of 2019, they are far short of being able to pass the Turing test.',\n",
              " '[2] Chatbots are typically used in dialog systems for various practical purposes including customer service or information acquisition.',\n",
              " 'Some chatbots use sophisticated natural language processing systems, but many simpler ones scan for keywords within the input, then pull a reply with the most matching keywords, or the most similar wording pattern, from a database.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPlKGngQ9Tv-",
        "colab_type": "code",
        "outputId": "44de76bd-0e9b-4ee0-957f-583de58337e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_sentence = 'Hej co tam! halo? Czesc..'\n",
        "LemNormalize(test_sentence)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hej', 'co', 'tam', 'halo', 'czesc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJvR2JN49TwF",
        "colab_type": "text"
      },
      "source": [
        "##### Next, we shall define a function for a greeting by the bot i.e if a user’s input is a greeting, \n",
        "the bot shall return a greeting response.ELIZA uses a simple keyword matching for greetings.\n",
        "We will utilize the same concept here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-1TJqVX9TwJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GREETING_INPUTS = [\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",]\n",
        "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
        "def greeting(sentence):\n",
        " \n",
        "    for word in sentence.split():\n",
        "        if word.lower() in GREETING_INPUTS:\n",
        "            return random.choice(GREETING_RESPONSES)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VGIzKRe9TwT",
        "colab_type": "text"
      },
      "source": [
        " ##### From scikit learn library, import the TFidf vectorizer to convert a collection of raw documents to a matrix of TF-IDF features. cosine_similarity will be used to find the similarity between words entered by the user and the words in the corpus. This is the simplest possible implementation of a chatbot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BO02cbN59TwW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_KDPLQu9Twd",
        "colab_type": "text"
      },
      "source": [
        "### TfidfVectorizer\n",
        "#### Term Frequency (TF)\n",
        "##### ilosc występowan danego słowa podzielona na ilosc wszystkich słów\n",
        "#### Inverse Data Frequency (IDF)\n",
        "##### mierzy ważnośc danego słowa, podczs obliczania TF każde słowo jest traktowane tak samo, Jednak w tekscie często pojawiają się słowa tj. \"of\", \"is\" a nie są ważne. Dlatego musimy zmiejszyć wagę czesto pojawiających się słów i zwiekszyć wagę słów pojawiających się rzadko. Wzor = log(liczba_wszystkich_dokumentów/liczba_dokumentów_z_danym_słowem) \n",
        "##### Example:\n",
        "##### Consider a document containing 100 words wherein the word cat appears 3 times. The term frequency (i.e., tf) for cat is then (3 / 100) = 0.03. Now, assume we have 10 million documents and the word cat appears in one thousand of these. Then, the inverse document frequency (i.e., idf) is calculated as log(10,000,000 / 1,000) = 4. Thus, the Tf-idf weight is the product of these quantities: 0.03 * 4 = 0.12.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-r7Z6AdD9Twh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def response(user_response):\n",
        "    robo_response=''\n",
        "    sent_tokens.append(user_response)\n",
        "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
        "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
        "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "    idx=vals.argsort()[0][-2]\n",
        "    flat = vals.flatten()\n",
        "    flat.sort()\n",
        "    req_tfidf = flat[-2]\n",
        "    if(req_tfidf==0):\n",
        "        robo_response=robo_response+\"I am sorry! I don't understand you\"\n",
        "        return robo_response\n",
        "    else:\n",
        "        robo_response = robo_response+sent_tokens[idx]\n",
        "        return robo_response"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5-0xoN79Twm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hr54Xh799Tws",
        "colab_type": "code",
        "outputId": "1e593c4d-4b9f-4f29-f8b2-ce3edf7add61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        }
      },
      "source": [
        "flag=True\n",
        "print(\"ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye!\")\n",
        "while(flag==True):\n",
        "    user_response = input()\n",
        "    user_response=user_response.lower()\n",
        "    if(user_response!='bye'):\n",
        "        if(user_response=='thanks' or user_response=='thank you' ):\n",
        "            flag=False\n",
        "            print(\"ROBO: You are welcome..\")\n",
        "        else:\n",
        "            if(greeting(user_response)!=None):\n",
        "                print(\"ROBO: \"+greeting(user_response))\n",
        "            else:\n",
        "                print(\"ROBO: \",end=\"\")\n",
        "                print(response(user_response))\n",
        "                sent_tokens.remove(user_response)\n",
        "    else:\n",
        "        flag=False\n",
        "        print(\"ROBO: Bye! take care..\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}